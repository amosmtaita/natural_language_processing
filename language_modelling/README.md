# Language Modelling 

## Overview
We include different notebooks to provide the student with practical means to explore language models and transformers, as well as their application in NLP tasks like wotd-sense disambiguation. We follow an incremental approach in accordance with the contents of session 6 of the course, focused on semantics.

## N-gram based language models
In [this notebook](https://github.com/acastellanos-ie/natural_language_processing/blob/master/language_modelling/language%20modelling.ipynb) we are going to start playing with languages models. In particular, we are going to start with the simplest approach based on n-grams. Then, in the following threads, we will move to more advanced approaches based on LSTM and Transformer architectures.

## Fine-tuning pre-trained transformers for text classification
Fine-tuning pre-trained language models learnt with transformers has improved the state of the art in multiple NLP evaluation tasks. In [this notebook](https://github.com/hybridnlp/tutorial/blob/master/01a_nlm_and_contextual_embeddings.ipynb) we fine tune a pre-trained  BERT language model to carry out a binary classification task where tweets are labelled as generated by bots or hurmans.

## Using transformers for word-sense disambiguation
In [this notebook](https://github.com/hybridnlp/tutorial/blob/master/03a_LMMS_Transigrafo.ipynb) we show an implementation of Transigrafo as a extension of the code for Language Modelling Makes Sense (LMMS), presented at ACL 2019, which uses contextual embeddings in Word Sense Disambiguation (WSD) tasks.


