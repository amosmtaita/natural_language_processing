{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AraBERT-Trainer-Optuna-SA",
      "provenance": [],
      "collapsed_sections": [
        "5EZF6bJbtq5X",
        "MA5607R9jU3a",
        "AANFYzSruvH9"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/natural_language_processing/blob/master/text_classification_practice_dl/AraBERT_Trainer_Optuna_SA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4mZ8KYblg-g"
      },
      "source": [
        "#installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4tlwQ8si_FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088d7b24-ae0e-4709-8405-729b6151bd7d"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Thu Apr 22 13:22:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y024z5AnlTLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764e9746-9704-4497-d6b9-93efab106b1c"
      },
      "source": [
        "!pip install optuna==2.3.0\n",
        "!pip install transformers==4.2.1\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!git clone https://github.com/aub-mind/arabert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optuna==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 21.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 61kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 71kB 10.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 81kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 9.3MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (20.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (4.41.1)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/d6/7d9acb68a77acd140be7fececb7f2701b2a29d2da9c54184cb8f93509590/cliff-3.7.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.19.5)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/a4/97eb6273839655cac14947986fa7a5935350fcfd4fff872e9654264c82d8/alembic-1.5.8-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 15.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna==2.3.0) (2.4.7)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (2.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (3.13)\n",
            "Collecting cmd2>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/15061b32332bb35ea2a2f6263d0f616779d576e82739ec8e7fcf3c94abf5/cmd2-1.5.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 9.0MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 19.5MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==2.3.0) (2.8.1)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (3.10.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna==2.3.0) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (20.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==2.3.0) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna==2.3.0) (3.4.1)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp37-none-any.whl size=359761 sha256=41b1526629b637b2383561aaf4ef7a21ac981acad855787e4cbdcd4a2e6c7b15\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11107 sha256=a716fc1b16e174c5a4214a0f2df836fffa05801b7e6f2ce64dac718f6313cb64\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, colorama, cmd2, pbr, stevedore, cliff, colorlog, cmaes, python-editor, Mako, alembic, optuna\n",
            "Successfully installed Mako-1.1.4 alembic-1.5.8 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorama-0.4.4 colorlog-5.0.1 optuna-2.3.0 pbr-5.5.1 pyperclip-1.8.2 python-editor-1.0.4 stevedore-3.3.0\n",
            "Collecting transformers==4.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (3.10.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.1) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.1) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.2.1\n",
            "Collecting farasapy\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/69/39ead86569e95141eede67703a93585be8454a801b7aab93ad1a7f0a8375/farasapy-0.0.13-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farasapy) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.13\n",
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp37-none-any.whl size=113324 sha256=d0052457bd22281a0d59e9c47d59bc725b23a4efb3ae4ee4a7710e39015a0634\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.10\n",
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 527, done.\u001b[K\n",
            "remote: Counting objects: 100% (313/313), done.\u001b[K\n",
            "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
            "remote: Total 527 (delta 165), reused 225 (delta 82), pack-reused 214\u001b[K\n",
            "Receiving objects: 100% (527/527), 4.86 MiB | 16.36 MiB/s, done.\n",
            "Resolving deltas: 100% (288/288), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlMoq-hmZ61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70211388-b27f-4633-b693-63ce04e74d3a"
      },
      "source": [
        "!git clone https://github.com/elnagara/HARD-Arabic-Dataset\n",
        "!git clone https://github.com/mahmoudnabil/ASTD\n",
        "!git clone https://github.com/nora-twairesh/AraSenti\n",
        "!git clone https://github.com/mohamedadaly/LABR\n",
        "!wget http://homepages.inf.ed.ac.uk/wmagdy/Resources/ArSAS.zip\n",
        "!unzip ArSAS.zip\n",
        "!unrar x '/content/HARD-Arabic-Dataset/data/unbalanced-reviews.rar'\n",
        "!unzip '/content/HARD-Arabic-Dataset/data/balanced-reviews.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'HARD-Arabic-Dataset'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Total 100 (delta 0), reused 0 (delta 0), pack-reused 100\u001b[K\n",
            "Receiving objects: 100% (100/100), 116.36 MiB | 16.75 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "Cloning into 'ASTD'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Total 29 (delta 0), reused 0 (delta 0), pack-reused 29\u001b[K\n",
            "Unpacking objects: 100% (29/29), done.\n",
            "Cloning into 'AraSenti'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n",
            "Cloning into 'LABR'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Total 37 (delta 0), reused 0 (delta 0), pack-reused 37\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n",
            "--2021-04-22 13:23:01--  http://homepages.inf.ed.ac.uk/wmagdy/Resources/ArSAS.zip\n",
            "Resolving homepages.inf.ed.ac.uk (homepages.inf.ed.ac.uk)... 129.215.32.113\n",
            "Connecting to homepages.inf.ed.ac.uk (homepages.inf.ed.ac.uk)|129.215.32.113|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://homepages.inf.ed.ac.uk/wmagdy/Resources/ArSAS.zip [following]\n",
            "--2021-04-22 13:23:01--  https://homepages.inf.ed.ac.uk/wmagdy/Resources/ArSAS.zip\n",
            "Connecting to homepages.inf.ed.ac.uk (homepages.inf.ed.ac.uk)|129.215.32.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1905723 (1.8M) [application/zip]\n",
            "Saving to: ‘ArSAS.zip’\n",
            "\n",
            "ArSAS.zip           100%[===================>]   1.82M  1.88MB/s    in 1.0s    \n",
            "\n",
            "2021-04-22 13:23:02 (1.88 MB/s) - ‘ArSAS.zip’ saved [1905723/1905723]\n",
            "\n",
            "Archive:  ArSAS.zip\n",
            "  inflating: ArSAS..txt              \n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/HARD-Arabic-Dataset/data/unbalanced-reviews.rar\n",
            "\n",
            "Extracting  unbalanced-reviews.txt                                       \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "Archive:  /content/HARD-Arabic-Dataset/data/balanced-reviews.zip\n",
            "  inflating: balanced-reviews.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krBvefg6l6vv"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVay9KamnC3I"
      },
      "source": [
        "#Creating training datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr84ozGinCFh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "all_datasets= []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PhWP2JzrEci"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name,\n",
        "        train,\n",
        "        test,\n",
        "        label_list,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WYy5ikAs7l3"
      },
      "source": [
        "DATA_COLUMN = \"text\"\n",
        "LABEL_COLUMN = \"label\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joWPYWGMqLau"
      },
      "source": [
        "##HARD - Balanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6oL3qkXmOgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3047e51-11c1-4a13-ac6e-acc1658d977d"
      },
      "source": [
        "df_HARD = pd.read_csv(\"/content/balanced-reviews.txt\", sep=\"\\t\", header=0,encoding='utf-16')\n",
        "\n",
        "df_HARD = df_HARD[[\"review\",\"rating\"]]  # we are interested in rating and review only\n",
        "df_HARD.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "print(df_HARD[LABEL_COLUMN].value_counts())\n",
        "# code rating as +ve if > 3, -ve if less, no 3s in dataset\n",
        "\n",
        "hard_map = {\n",
        "    5: 'POS',\n",
        "    4: 'POS',\n",
        "    2: 'NEG',\n",
        "    1: 'NEG'\n",
        "}\n",
        "\n",
        "df_HARD[LABEL_COLUMN] = df_HARD[LABEL_COLUMN].apply(lambda x: hard_map[x])\n",
        "train_HARD, test_HARD = train_test_split(df_HARD, test_size=0.2, random_state=42)\n",
        "label_list_HARD = ['NEG', 'POS']\n",
        "\n",
        "data_Hard = Dataset(\"HARD\", train_HARD, test_HARD, label_list_HARD)\n",
        "all_datasets.append(data_Hard)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2    38467\n",
            "4    26450\n",
            "5    26399\n",
            "1    14382\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sycNXzvgr7BZ"
      },
      "source": [
        "##ASTD- Unbalanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wapHixOBrq5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c6c059-ca68-412b-df02-9f004a93839e"
      },
      "source": [
        "df_ASTD_UN = pd.read_csv(\n",
        "    \"/content/ASTD/data/Tweets.txt\", sep=\"\\t\", header=None\n",
        ")\n",
        "\n",
        "df_ASTD_UN.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "\n",
        "df_ASTD_UN = df_ASTD_UN[df_ASTD_UN[LABEL_COLUMN]!= 'OBJ']\n",
        "\n",
        "train_ASTD_UN, test_ASTD_UN = train_test_split(\n",
        "    df_ASTD_UN, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "label_list_ASTD_UN = list(df_ASTD_UN[LABEL_COLUMN].unique())\n",
        "print(label_list_ASTD_UN)\n",
        "print(df_ASTD_UN[LABEL_COLUMN].value_counts())\n",
        "\n",
        "data_ASTD_UN = Dataset(\n",
        "    \"ASTD-Unbalanced\", train_ASTD_UN, test_ASTD_UN, label_list_ASTD_UN\n",
        ")\n",
        "\n",
        "all_datasets.append(data_ASTD_UN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['POS', 'NEG', 'NEUTRAL']\n",
            "NEG        1642\n",
            "NEUTRAL     805\n",
            "POS         777\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgmdEZhgssMM"
      },
      "source": [
        "##ASTD-Dahou-Balanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Dss2k4sPMo"
      },
      "source": [
        "df_ASTD_B = pd.read_csv(\n",
        "    \"/content/drive/My Drive/Datasets/Dahou/data_csv_balanced/ASTD-balanced-not-linked.csv\",\n",
        "    sep=\",\",\n",
        "    header=0,\n",
        ")\n",
        "\n",
        "df_ASTD_B.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "\n",
        "df_ASTD_B[LABEL_COLUMN] = df_ASTD_B[LABEL_COLUMN].apply(lambda x: 'NEG' if (x == -1) else 'POS')\n",
        "\n",
        "train_ASTD_B, test_ASTD_B = train_test_split(df_ASTD_B, test_size=0.2, random_state=42)\n",
        "label_list_ASTD_B = list(df_ASTD_B[LABEL_COLUMN].unique())\n",
        "\n",
        "data_ASTD_B = Dataset(\n",
        "    \"ASTD-Dahou-Balanced\", train_ASTD_B, test_ASTD_B, label_list_ASTD_B\n",
        ")\n",
        "all_datasets.append(data_ASTD_B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EZF6bJbtq5X"
      },
      "source": [
        "##Arsentv-lev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HIHvLQPtRZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17705fb7-4a29-4dee-edfc-696d0a9bef62"
      },
      "source": [
        "df_ArSenTD = pd.read_csv(\n",
        "    \"/content/drive/My Drive/Datasets/ArSenTD-LEV/ArSenTD-LEV.tsv\", sep=\"\\t\", header=0\n",
        ")\n",
        "\n",
        "df_ArSenTD = df_ArSenTD[['Tweet','Sentiment']]\n",
        "\n",
        "df_ArSenTD.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "\n",
        "print(df_ArSenTD[LABEL_COLUMN].value_counts())\n",
        "label_list_ArSenTD = list(df_ArSenTD[LABEL_COLUMN].unique())\n",
        "\n",
        "train_ArSenTD, test_ArSenTD = train_test_split(\n",
        "    df_ArSenTD, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "data_ArSenTD = Dataset(\"ArSenTD-LEV\", train_ArSenTD, test_ArSenTD, label_list_ArSenTD)\n",
        "all_datasets.append(data_ArSenTD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative         1253\n",
            "neutral           885\n",
            "positive          835\n",
            "very_negative     630\n",
            "very_positive     397\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA5607R9jU3a"
      },
      "source": [
        "##AJGT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bww9o0i7uQSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1176f0cc-450b-401d-c6de-9a88dbb7b7f2"
      },
      "source": [
        "df_AJGT = pd.read_excel(\"/content/drive/My Drive/Datasets/Ajgt/AJGT.xlsx\", header=0)\n",
        "\n",
        "df_AJGT = df_AJGT[[\"Feed\", \"Sentiment\"]]\n",
        "df_AJGT.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "\n",
        "\n",
        "\n",
        "train_AJGT, test_AJGT = train_test_split(df_AJGT, test_size=0.2, random_state=42)\n",
        "\n",
        "print(df_AJGT[LABEL_COLUMN].value_counts())\n",
        "label_list_AJGT = list(df_AJGT[LABEL_COLUMN].unique())\n",
        "\n",
        "data_AJGT = Dataset(\"AJGT\", train_AJGT, test_AJGT, label_list_AJGT)\n",
        "all_datasets.append(data_AJGT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative    900\n",
            "Positive    900\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AANFYzSruvH9"
      },
      "source": [
        "##LABR-Unbalanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqig9TYmtac8",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551015f9-ba6a-48d0-c394-fa049c4ae51f"
      },
      "source": [
        "#@title\n",
        "%%writefile labr.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Mar 10 16:27:03 2013\n",
        "\n",
        "@author: Mohamed Aly <mohamed@mohamedaly.info>\n",
        "\"\"\"\n",
        "\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "class LABR:\n",
        "    def __init__(self):\n",
        "        self.REVIEWS_PATH = \"LABR/data/\"\n",
        "        self.RAW_REVIEWS_FILE = \"raw_reviews.tsv\"\n",
        "        self.DELETED_REVIEWS_FILE = \"deleted_reviews.tsv\"\n",
        "        self.CLEAN_REVIEWS_FILE = \"reviews.tsv\"\n",
        "\n",
        "    # Copied from the PyArabic package.\n",
        "    def arabicrange(self):\n",
        "        \"\"\"return a list of arabic characteres .\n",
        "        Return a list of characteres between \\u060c to \\u0652\n",
        "        @return: list of arabic characteres.\n",
        "        @rtype: unicode;\n",
        "        \"\"\"\n",
        "        mylist=[];\n",
        "        for i in range(0x0600, 0x00653):\n",
        "            try :\n",
        "                mylist.append(unichr(i));\n",
        "            except ValueError:\n",
        "                pass;\n",
        "        return mylist;\n",
        "\n",
        "    # cleans a single review\n",
        "    def clean_raw_review(self, body):\n",
        "         # patterns to remove first\n",
        "        pat = [\\\n",
        "            (u'http[s]?://[a-zA-Z0-9_\\-./~\\?=%&]+', u''),               # remove links\n",
        "            (u'www[a-zA-Z0-9_\\-?=%&/.~]+', u''),\n",
        "#            u'\\n+': u' ',                     # remove newlines\n",
        "            (u'<br />', u' '),                  # remove html line breaks\n",
        "            (u'</?[^>]+>', u' '),              # remove html markup\n",
        "#            u'http': u'',\n",
        "            (u'[a-zA-Z]+\\.org', u''),\n",
        "            (u'[a-zA-Z]+\\.com', u''),\n",
        "            (u'://', u''),\n",
        "            (u'&[^;]+;', u' '),\n",
        "            (u':D', u':)'),\n",
        "#            (u'[0-9/]+', u''),\n",
        "#            u'[a-zA-Z.]+': u'',\n",
        "#            u'[^0-9' + u''.join(self.arabicrange()) + \\\n",
        "#                u\"!.,;:$%&*%'#(){}~`\\[\\]/\\\\\\\\\\\"\" + \\\n",
        "#                u'\\s^><\\-_\\u201D\\u00AB=\\u2026]+': u'',          # remove latin characters\n",
        "            (u'\\s+', u' '),                     # remove spaces\n",
        "            (u'\\.+', u'.'),                     # multiple dots\n",
        "            (u'[\\u201C\\u201D]', u'\"'),          #“\n",
        "            (u'[\\u2665\\u2764]', u''),           # heart symbol\n",
        "            (u'[\\u00BB\\u00AB]', u'\"'),\n",
        "            (u'\\u2013', u'-'),                # dash\n",
        "        ]\n",
        "\n",
        "        # patterns that disqualify a review\n",
        "        remove_if_there = [\\\n",
        "            (u'[^0-9' + u''.join(self.arabicrange()) + \\\n",
        "                u\"!.,;:$%&*%'#(){}~`\\[\\]/\\\\\\\\\\\"\" + \\\n",
        "                u'\\s\\^><\\-_\\u201D\\u00AB=\\u2026+|' + \\\n",
        "                u'\\u0660-\\u066D\\u201C\\u201D' + \\\n",
        "                u'\\ufefb\\ufef7\\ufef5\\ufef9]+', u''),                   # non arabic characters\n",
        "        ]\n",
        "\n",
        "        # patterns that disqualify if empty after removing\n",
        "        remove_if_empty_after = [\\\n",
        "            (u'[0-9a-zA-Z\\-_]', u' '),             #alpha-numeric\n",
        "            (u'[0-9' + u\".,!;:$%&*%'#(){}~`\\[\\]/\\\\\\\\\\\"\" + \\\n",
        "                u'\\s\\^><`\\-=_+]+', u''),                  # remove just punctuation\n",
        "            (u'\\s+', u' '),                     # remove spaces\n",
        "        ]\n",
        "\n",
        "        # remove again\n",
        "        # patterns to remove\n",
        "        pat2 = [\\\n",
        "#            u'[^0-9' + u''.join(self.arabicrange()) + \\\n",
        "#                u\"!.,;:$%&*%'#(){}~`\\[\\]/\\\\\\\\\\\"\" + \\\n",
        "#                u'\\s^><\\-_\\u201D\\u00AB=\\u2026]+': u'',          # remove latin characters\n",
        "        ]\n",
        "\n",
        "        skip = False\n",
        "\n",
        "        # if empty body, skip\n",
        "        if body == u'': skip = True\n",
        "\n",
        "        # do some subsitutions\n",
        "        for k,v in pat:\n",
        "            body = re.sub(k, v, body)\n",
        "\n",
        "        # remove if exist\n",
        "        for k,v in remove_if_there:\n",
        "            if re.search(k, body):\n",
        "                skip = True\n",
        "\n",
        "        # remove if empty after replacing\n",
        "        for k,v in remove_if_empty_after:\n",
        "            temp = re.sub(k, v, body)\n",
        "            if temp == u\" \" or temp == u\"\":\n",
        "                skip = True\n",
        "\n",
        "        # do some more subsitutions\n",
        "        if not skip:\n",
        "            for k,v in pat2:\n",
        "                body = re.sub(k, v, body)\n",
        "\n",
        "        # if empty string, skip\n",
        "        if body == u'' or body == u' ':\n",
        "            skip = True\n",
        "\n",
        "        if not skip:\n",
        "            return body\n",
        "        else:\n",
        "            return u\"\"\n",
        "\n",
        "    # Read raw reviews from file and clean and write into clean_reviews\n",
        "    def clean_raw_reviews(self):\n",
        "        # input file\n",
        "        in_file = codecs.open(self.REVIEWS_PATH + self.RAW_REVIEWS_FILE,\n",
        "                              'r', encoding=\"utf-8\")\n",
        "        reviews = in_file.readlines()\n",
        "\n",
        "        # Output file: rating<tab>content\n",
        "        out_file = open(self.REVIEWS_PATH + self.CLEAN_REVIEWS_FILE,\n",
        "                        'w', buffering = 100)\n",
        "        deleted_file = open(self.REVIEWS_PATH + self.DELETED_REVIEWS_FILE,\n",
        "                            'w', buffering = 100)\n",
        "\n",
        "        counter = 1\n",
        "        for i in xrange(0, len(reviews)):\n",
        "            review = reviews[i]\n",
        "            skip = False\n",
        "\n",
        "#           # If line starts with #, then skip\n",
        "#            if review[0] == u\"#\": continue\n",
        "\n",
        "            # split by <tab>\n",
        "            parts = review.split(u\"\\t\")\n",
        "\n",
        "            # rating is first part and body is last part\n",
        "            rating = parts[0]\n",
        "            review_id = parts[1]\n",
        "            user_id = parts[2]\n",
        "            book_id = parts[3]\n",
        "            body = parts[4].strip()\n",
        "\n",
        "            # clean body\n",
        "            body = self.clean_raw_review(body)\n",
        "            if body == u\"\": skip = True\n",
        "\n",
        "            if i % 5000 == 0:\n",
        "                print(\"review %d:\" % (i))\n",
        "\n",
        "            # write output\n",
        "            line = u\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (rating, review_id, user_id,\n",
        "                                              book_id, body)\n",
        "            if not skip:\n",
        "                out_file.write(line.encode('utf-8'))\n",
        "                counter += 1\n",
        "            else:\n",
        "                deleted_file.write(line.encode('utf-8'))\n",
        "\n",
        "    # Read the reviews file. Returns a tuple containing these lists:\n",
        "    #   rating: the rating 1 -> 5\n",
        "    #   review_id: the id of the review\n",
        "    #   user_id: the id of the user\n",
        "    #   book_id: the id of the book\n",
        "    #   body: the text of the review\n",
        "    def read_review_file(self, file_name):\n",
        "        reviews = codecs.open(file_name, 'r', 'utf-8').readlines()\n",
        "\n",
        "        # remove comment lines and newlines\n",
        "        reviews = [r.strip() for r in reviews if r[0] != u'#']\n",
        "\n",
        "        # parse\n",
        "        rating = list()\n",
        "        review_id = list()\n",
        "        user_id = list()\n",
        "        book_id = list()\n",
        "        body = list()\n",
        "        for review in reviews:\n",
        "            # split by <tab>\n",
        "            parts = review.split(u\"\\t\")\n",
        "\n",
        "            # rating is first part and body is last part\n",
        "            rating.append(int(parts[0]))\n",
        "            review_id.append(parts[1])\n",
        "            user_id.append(parts[2])\n",
        "            book_id.append(parts[3])\n",
        "            if len(parts) > 4:\n",
        "                body.append(parts[4])\n",
        "            else:\n",
        "                body.append(u\"\")\n",
        "\n",
        "        return (rating, review_id, user_id, book_id, body)\n",
        "\n",
        "    # Writes reviews to a file\n",
        "    def write_review_file(self, file_name, rating, review_id, user_id,\n",
        "                          book_id, body):\n",
        "\n",
        "        lines = list()\n",
        "        # loop\n",
        "        for i in xrange(len(rating)):\n",
        "            line = u\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (rating[i], review_id[i],\n",
        "                                              user_id[i], book_id[i],\n",
        "                                              body[i])\n",
        "            lines.append(line)\n",
        "\n",
        "        open(file_name, 'w').write(u''.join(lines).encode('utf-8'))\n",
        "\n",
        "    def read_clean_reviews(self):\n",
        "        return self.read_review_file(self.REVIEWS_PATH +\n",
        "                                     self.CLEAN_REVIEWS_FILE)\n",
        "\n",
        "    def read_raw_reviews(self):\n",
        "        return self.read_review_file(self.REVIEWS_PATH + self.RAW_REVIEWS_FILE)\n",
        "\n",
        "    # Splits the dataset into a training/test sets in the setting of using 5\n",
        "    # classes (predicting the rating value from 1 to 5)\n",
        "    def split_train_test_5class(self, rating, percent_test,\n",
        "                                balanced = \"unbalanced\"):\n",
        "        np.random.seed(1234)\n",
        "\n",
        "        num_reviews = len(rating)\n",
        "        review_ids = np.arange(0, num_reviews)\n",
        "\n",
        "        if balanced == \"unbalanced\":\n",
        "            ntest = np.floor(num_reviews * percent_test)\n",
        "            np.random.shuffle(review_ids)\n",
        "\n",
        "            test_ids = review_ids[:ntest]\n",
        "            train_ids = review_ids[ntest:]\n",
        "\n",
        "        elif balanced == \"balanced\":\n",
        "            (sizes, bins) = np.histogram(rating, [1, 2, 3, 4, 5, 6])\n",
        "            min_size = np.min(sizes)\n",
        "            print(min_size)\n",
        "\n",
        "            # sample review ids equally among classes\n",
        "            test_ids = np.zeros((0,), dtype=\"int32\")\n",
        "            train_ids = np.zeros((0,), dtype=\"int32\")\n",
        "            rating = np.array(rating)\n",
        "            ntest = np.floor(min_size * percent_test)\n",
        "            for c in range(1, 6):\n",
        "                cids = review_ids[np.nonzero(rating == c)]\n",
        "                np.random.shuffle(cids)\n",
        "\n",
        "                test_ids = np.r_[test_ids, cids[:ntest]]\n",
        "                train_ids = np.r_[train_ids, cids[ntest:min_size]]\n",
        "\n",
        "        train_file = self.REVIEWS_PATH + \"5class-\" + balanced+ \"-train.txt\"\n",
        "        test_file = self.REVIEWS_PATH + \"5class-\" + balanced+ \"-test.txt\"\n",
        "\n",
        "        open(train_file, 'w').write('\\n'.join(map(str, train_ids)))\n",
        "        open(test_file, 'w').write('\\n'.join(map(str, test_ids)))\n",
        "\n",
        "        return (train_ids, test_ids)\n",
        "\n",
        "    # Splits the dataset into a training/test sets in the setting of using 2\n",
        "    # classes (predicting the polarity of the review where ratings 1 & 2\n",
        "    # are considered negative, ratings 4 & 5 are positive, and rating 3 is\n",
        "    # ignored)\n",
        "    def split_train_test_2class(self, rating, percent_test,\n",
        "                                balanced = \"unbalanced\"):\n",
        "        np.random.seed(1234)\n",
        "\n",
        "        rating = np.array(rating, dtype='int32')\n",
        "        # length\n",
        "        num_reviews = len(rating)\n",
        "        review_ids = np.arange(0, num_reviews)\n",
        "\n",
        "        # convert to binary, with ratings [1, 2] --> neg and [4, 5] --> pos\n",
        "        rating[rating == 2] = 1\n",
        "        rating[rating == 4] = 5\n",
        "\n",
        "        ids = (rating == 1) + (rating == 5)\n",
        "        review_ids = review_ids[ids]\n",
        "        rating = rating[ids]\n",
        "        rating[rating == 1] = 0\n",
        "        rating[rating == 5] = 1\n",
        "\n",
        "        # get length after filtering\n",
        "        num_reviews = rating.shape[0]\n",
        "\n",
        "        if balanced == \"unbalanced\":\n",
        "            ntest = np.floor(num_reviews * percent_test)\n",
        "            np.random.shuffle(review_ids)\n",
        "\n",
        "            test_ids = review_ids[:ntest]\n",
        "            train_ids = review_ids[ntest:]\n",
        "\n",
        "        elif balanced == \"balanced\":\n",
        "            (sizes, bins) = np.histogram(rating, [0, 1, 2])\n",
        "            min_size = np.min(sizes)\n",
        "            print(min_size)\n",
        "\n",
        "            # sample review ids equally among classes\n",
        "            test_ids = np.zeros((0,), dtype=\"int32\")\n",
        "            train_ids = np.zeros((0,), dtype=\"int32\")\n",
        "            rating = np.array(rating)\n",
        "            ntest = np.floor(min_size * percent_test)\n",
        "            for c in [0, 1]:\n",
        "                cids = review_ids[np.nonzero(rating == c)]\n",
        "                np.random.shuffle(cids)\n",
        "\n",
        "                test_ids = np.r_[test_ids, cids[:ntest]]\n",
        "                train_ids = np.r_[train_ids, cids[ntest:min_size]]\n",
        "\n",
        "        train_file = self.REVIEWS_PATH + \"2class-\" + balanced+ \"-train.txt\"\n",
        "        test_file = self.REVIEWS_PATH + \"2class-\" + balanced+ \"-test.txt\"\n",
        "\n",
        "        open(train_file, 'w').write('\\n'.join(map(str, train_ids)))\n",
        "        open(test_file, 'w').write('\\n'.join(map(str, test_ids)))\n",
        "\n",
        "        return (train_ids, test_ids)\n",
        "\n",
        "    # Reads a training or test file. The file contains the indices of the\n",
        "    # reviews from the clean reviews file.\n",
        "    def read_train_test_file(self, file_name):\n",
        "        ins = open(file_name).readlines()\n",
        "        ins = [int(i.strip()) for i in ins]\n",
        "\n",
        "        return ins\n",
        "\n",
        "    # A helpter function.\n",
        "    def set_binary_klass(self, ar):\n",
        "        ar[(ar == 1) + (ar == 2)] = 0\n",
        "        ar[(ar == 4) + (ar == 5)] = 1\n",
        "\n",
        "    # Returns (train_x, train_y, test_x, test_y)\n",
        "    # where x is the review body and y is the rating (1->5 or 0->1)\n",
        "    def get_train_test(self, klass = \"2\", balanced = \"balanced\"):\n",
        "        (rating, a, b, c, body) = self.read_clean_reviews()\n",
        "        rating = np.array(rating)\n",
        "        body = pd.Series(body)\n",
        "\n",
        "        train_file = (self.REVIEWS_PATH + klass + \"class-\" +\n",
        "            balanced+ \"-train.txt\")\n",
        "        test_file = (self.REVIEWS_PATH + klass + \"class-\" +\n",
        "            balanced+ \"-test.txt\")\n",
        "\n",
        "        train_ids = self.read_train_test_file(train_file)\n",
        "        test_ids = self.read_train_test_file(test_file)\n",
        "\n",
        "        train_y = rating[train_ids]\n",
        "        test_y = rating[test_ids]\n",
        "        train_x = body[train_ids]\n",
        "        test_x = body[test_ids]\n",
        "\n",
        "        if klass == \"2\":\n",
        "            self.set_binary_klass(train_y)\n",
        "            self.set_binary_klass(test_y)\n",
        "\n",
        "        return (train_x, train_y, test_x, test_y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing labr.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-HpWYH1umBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597344e0-7bd9-44be-b600-f8722b873f03"
      },
      "source": [
        "from labr import LABR\n",
        "\n",
        "labr_helper = LABR()\n",
        "\n",
        "(d_train, y_train, d_test, y_test) = labr_helper.get_train_test(\n",
        "    klass=\"2\", balanced=\"unbalanced\"\n",
        ")\n",
        "\n",
        "train_LABR_B_U = pd.DataFrame({DATA_COLUMN: d_train, LABEL_COLUMN: y_train})\n",
        "test_LABR_B_U = pd.DataFrame({DATA_COLUMN: d_test, LABEL_COLUMN: y_test})\n",
        "\n",
        "train_LABR_B_U[LABEL_COLUMN] = train_LABR_B_U[LABEL_COLUMN].apply(lambda x: 'NEG' if (x == 0) else 'POS')\n",
        "test_LABR_B_U[LABEL_COLUMN] = test_LABR_B_U[LABEL_COLUMN].apply(lambda x: 'NEG' if (x == 0) else 'POS')\n",
        "\n",
        "print(train_LABR_B_U[LABEL_COLUMN].value_counts() + test_LABR_B_U[LABEL_COLUMN].value_counts())\n",
        "label_list_LABR_B_U = list(test_LABR_B_U[LABEL_COLUMN].unique())\n",
        "\n",
        "data_LABR_B_U = Dataset(\n",
        "    \"LABR-UN-Binary\", train_LABR_B_U, test_LABR_B_U, label_list_LABR_B_U\n",
        ")\n",
        "all_datasets.append(data_LABR_B_U)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POS    42832\n",
            "NEG     8224\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFYs8DEEvHoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac932c0-16e0-4cc0-f7b6-0ad70ba9c197"
      },
      "source": [
        "for x in all_datasets:\n",
        "  print(x.name) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HARD\n",
            "ASTD-Unbalanced\n",
            "ASTD-Dahou-Balanced\n",
            "ArSenTD-LEV\n",
            "AJGT\n",
            "LABR-UN-Binary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcwdslw7v0Q8"
      },
      "source": [
        "#Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUn2RB6Bvrxj"
      },
      "source": [
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import logging\n",
        "import torch\n",
        "import optuna "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfNKr05tv7cA"
      },
      "source": [
        "logging.basicConfig(level=logging.WARNING)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4SGYoB2EDJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e82ed5a-0f54-45f2-f866-bd54c1bee75c"
      },
      "source": [
        "for x in all_datasets:\n",
        "  print(x.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HARD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Lma6tT5zJi"
      },
      "source": [
        "You can choose which model, and dataset from here along with the max sentence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzeVFoz1wDYf"
      },
      "source": [
        "dataset_name = 'HARD'\n",
        "model_name = 'aubmindlab/bert-base-arabertv02'\n",
        "task_name = 'classification'\n",
        "max_len = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ieCOj90aw8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd57d7a-453a-49af-ccf2-efc3a6dcb4ae"
      },
      "source": [
        "for d in all_datasets:\n",
        "  if d.name==dataset_name:\n",
        "    selected_dataset = d\n",
        "    print('Dataset found')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HARD\n",
            "Dataset found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt_lGy85zuca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004dafd1-0716-49dc-decf-03b54609a752"
      },
      "source": [
        "arabert_prep = ArabertPreprocessor(model_name.split(\"/\")[-1])\n",
        "\n",
        "selected_dataset.train[DATA_COLUMN] = selected_dataset.train[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(x))\n",
        "selected_dataset.test[DATA_COLUMN] = selected_dataset.test[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(x))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoyaRIGYCEzh",
        "outputId": "38504b91-5d68-4624-b957-e3697d9830a8"
      },
      "source": [
        "selected_dataset.test[DATA_COLUMN]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "555     علم من أسماء الموقوفين في السعودية حتى الآن : ...\n",
              "3491    يوم هادئ كان عبارة عن كم ايميل يتم إرساله من ع...\n",
              "527     الطائف يذكرني بايفون 4 ما صار ينزل له تحديث او...\n",
              "3925    بالأمس # حسن بلا صراخ و # ايران تغلق مكاتب لمؤ...\n",
              "2989    حكومة اقليم # كردستان حذرت اليوم من دفع الوضع ...\n",
              "                              ...                        \n",
              "1922    والله يا شباب ما خليتو اشي للبنات اذا واحد مفك...\n",
              "865     لا تسأل الاعزب متى يتزوج ، ولا العاطل متى يشتغ...\n",
              "3943    الانفعال السعودي من الصاروخ المبارك على الرياض...\n",
              "1642    [مستخدم] ممكن سؤال لما تكن كل هذا العداء والحق...\n",
              "2483    [مستخدم] مدون ؟ من وين لوين لولا الفيسبوك مين ...\n",
              "Name: text, Length: 800, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YS7XI2bZTyz"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\n",
        "      super(BERTDataset).__init__()\n",
        "      self.text = text\n",
        "      self.target = target\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "      \n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "        \n",
        "      input_ids = self.tokenizer.encode(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          truncation='longest_first'\n",
        "      )     \n",
        "    \n",
        "      attention_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = self.max_len - len(input_ids)\n",
        "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
        "      attention_mask = attention_mask + ([0] * padding_length)    \n",
        "      \n",
        "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mciZOFz-amkV"
      },
      "source": [
        "label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\n",
        "print(label_map)\n",
        "train_dataset = BERTDataset(selected_dataset.train[DATA_COLUMN].to_list(),selected_dataset.train[LABEL_COLUMN].to_list(),model_name,max_len,label_map)\n",
        "test_dataset = BERTDataset(selected_dataset.test[DATA_COLUMN].to_list(),selected_dataset.test[LABEL_COLUMN].to_list(),model_name,max_len,label_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt7l0IxjbmNu"
      },
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYU6G4vWc5nz"
      },
      "source": [
        "def compute_metrics(p): #p should be of type EvalPrediction\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  assert len(preds) == len(p.label_ids)\n",
        "  #print(classification_report(p.label_ids,preds))\n",
        "  #print(confusion_matrix(p.label_ids,preds))\n",
        "\n",
        "  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
        "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
        "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
        "  acc = accuracy_score(p.label_ids,preds)\n",
        "  return {\n",
        "      'macro_f1' : macro_f1,\n",
        "      'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
        "      'macro_precision': macro_precision,\n",
        "      'macro_recall': macro_recall,\n",
        "      'accuracy': acc\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnqiZOT37ByE"
      },
      "source": [
        "# HyperParameter Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJif-78C6Kmn"
      },
      "source": [
        "you can change the batch size and gradient accumulation from here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Z_kPB-dE_w"
      },
      "source": [
        "training_args = TrainingArguments(\"./train\")\n",
        "training_args.evaluate_during_training = True\n",
        "training_args.adam_epsilon = 1e-8\n",
        "training_args.lr_scheduler_type = 'cosine'\n",
        "training_args.fp16 = True\n",
        "training_args.per_device_train_batch_size = 16\n",
        "training_args.per_device_eval_batch_size = 16\n",
        "training_args.gradient_accumulation_steps = 2\n",
        "training_args.num_train_epochs= 8\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
        "# training_args.logging_steps = 200\n",
        "training_args.save_steps = 100000\n",
        "# training_args.save_steps = \n",
        "#training_args.eval_steps = \n",
        "training_args.disable_tqdm = True\n",
        "# print(\"Logging Step:\", training_args.logging_steps)\n",
        "# print(\"Eval Step:\",training_args.eval_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeqRACPYheeb"
      },
      "source": [
        "steps_per_epoch = (len(selected_dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "print(steps_per_epoch)\n",
        "print(total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8HOnY6odcwG"
      },
      "source": [
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset, \n",
        "    eval_dataset=test_dataset, \n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CHULcuX6a48"
      },
      "source": [
        "here you can define your search space.\n",
        "\n",
        "the `my_hp_space` function defines the hyper parameter super set, of which you can choose a subset (or even the whole set) for the grid search\n",
        "\n",
        "\n",
        "Note: You can include the opch count as a hyperparameter, but this will drasticly increase the search space, I prefer setting a fixed epcoh size, then I manually search for the highest score between the epochs since optuna can't do that as far as I know"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXwkOzRCdsIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d53ba88-0e12-422a-be98-65890efb75c9"
      },
      "source": [
        "def my_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 7e-5, step=1e-5),\n",
        "        \"seed\": trial.suggest_categorical(\"seed\", [0, 1, 42, 666, 123, 12345]),\n",
        "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\",0,total_steps*0.1,step=total_steps*0.1*0.5)\n",
        "    }\n",
        "\n",
        "search_space = {\n",
        "    \"learning_rate\":  list(np.arange(2e-5, 7e-5, 1e-5)),\n",
        "    \"seed\":  [0, 1, 42, 666, 123, 12345],\n",
        "    \"warmup_steps\": list(range(0, int((total_steps)*0.1)+1, int(total_steps*0.1*0.5)))\n",
        "}\n",
        "search_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': [2e-05,\n",
              "  3.0000000000000004e-05,\n",
              "  4.000000000000001e-05,\n",
              "  5.000000000000001e-05,\n",
              "  6.000000000000001e-05],\n",
              " 'seed': [0, 1, 42, 666, 123, 12345],\n",
              " 'warmup_steps': [0, 40, 80]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwcBc70Sfem3"
      },
      "source": [
        "def my_objective(metrics):\n",
        "    return metrics['eval_macro_f1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc4gOvtE66Mj"
      },
      "source": [
        "choose a study name to save it on disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSGLnBKv64sZ"
      },
      "source": [
        "name = \"sa-arabert-base-v2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l821sW7fdaY"
      },
      "source": [
        "best_run = trainer.hyperparameter_search(direction=\"maximize\",\n",
        "                                         hp_space=my_hp_space,\n",
        "                                         compute_objective=my_objective,\n",
        "                                         n_trials=None,\n",
        "                                         pruner=optuna.pruners.NopPruner(),\n",
        "                                         sampler=optuna.samplers.GridSampler(search_space),\n",
        "                                         study_name=name,\n",
        "                                         storage=\"sqlite:////content/drive/MyDrive/{}.db\".format(name),\n",
        "                                         load_if_exists=False # you can change this to true, for continuing the search\n",
        "                                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVwfM9ZJf12t"
      },
      "source": [
        "best_run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTmvFEs41WkV"
      },
      "source": [
        "#Regular Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_oGjIC-7Vow"
      },
      "source": [
        "This paert allows you to do a regular training with no hyper parameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xjs-X14uc0"
      },
      "source": [
        "training_args = TrainingArguments(\"./train\")\n",
        "training_args.evaluate_during_training = True\n",
        "training_args.adam_epsilon = 1e-8\n",
        "training_args.learning_rate = 5e-5\n",
        "training_args.fp16 = True\n",
        "training_args.per_device_train_batch_size = 16\n",
        "training_args.per_device_eval_batch_size = 16\n",
        "training_args.gradient_accumulation_steps = 2\n",
        "training_args.num_train_epochs= 8\n",
        "\n",
        "\n",
        "steps_per_epoch = (len(selected_dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "print(steps_per_epoch)\n",
        "print(total_steps)\n",
        "#Warmup_ratio\n",
        "warmup_ratio = 0.1\n",
        "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
        "\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
        "# training_args.logging_steps = 200\n",
        "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
        "training_args.seed = 42\n",
        "training_args.disable_tqdm = False\n",
        "training_args.lr_scheduler_type = 'cosine'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro5BW5ak4uc1"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = model_init(),\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx336O3J2SdQ"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Seyz8Yaj2ZCK"
      },
      "source": [
        "trainer.save_model(\"SOME_PATH\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}