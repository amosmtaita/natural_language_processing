{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AraBERT-Trainer-Optuna-SA",
      "provenance": [],
      "collapsed_sections": [
        "5EZF6bJbtq5X",
        "MA5607R9jU3a",
        "AANFYzSruvH9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/natural_language_processing/blob/master/text_classification_practice_dl/AraBERT_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coUhsrt20sOI"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**\n",
        "\n",
        "The first step is to clone the repository to have access to all the data and files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMgL9W4-0wXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953fc16a-9f52-4bd4-f915-3ac0b06bc0f7"
      },
      "source": [
        "! git clone https://github.com/acastellanos-ie/natural_language_processing.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'natural_language_processing' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gAhosq70ypp"
      },
      "source": [
        "Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy8O4JuL00hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb3e56b-3ca6-451a-a3c1-8489bbe4eaa5"
      },
      "source": [
        "! pip install -r natural_language_processing/arabic_requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 229 kB/s \n",
            "\u001b[?25hCollecting git+https://github.com/amaiya/eli5@tfkeras_0_10_1 (from -r natural_language_processing/arabic_requirements.txt (line 14))\n",
            "  Cloning https://github.com/amaiya/eli5 (to revision tfkeras_0_10_1) to /tmp/pip-req-build-ggs943tp\n",
            "  Running command git clone -q https://github.com/amaiya/eli5 /tmp/pip-req-build-ggs943tp\n",
            "  Running command git checkout -b tfkeras_0_10_1 --track origin/tfkeras_0_10_1\n",
            "  Switched to a new branch 'tfkeras_0_10_1'\n",
            "  Branch 'tfkeras_0_10_1' set up to track remote branch 'tfkeras_0_10_1' from 'origin'.\n",
            "Collecting git+https://github.com/amaiya/stellargraph@no_tf_dep_082 (from -r natural_language_processing/arabic_requirements.txt (line 15))\n",
            "  Cloning https://github.com/amaiya/stellargraph (to revision no_tf_dep_082) to /tmp/pip-req-build-kq9t7919\n",
            "  Running command git clone -q https://github.com/amaiya/stellargraph /tmp/pip-req-build-kq9t7919\n",
            "  Running command git checkout -b no_tf_dep_082 --track origin/no_tf_dep_082\n",
            "  Switched to a new branch 'no_tf_dep_082'\n",
            "  Branch 'no_tf_dep_082' set up to track remote branch 'no_tf_dep_082' from 'origin'.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r natural_language_processing/arabic_requirements.txt (line 1)) (3.2.5)\n",
            "Collecting camel-tools\n",
            "  Using cached camel_tools-1.1.0.tar.gz (56 kB)\n",
            "Collecting stanza\n",
            "  Using cached stanza-1.2.2-py3-none-any.whl (337 kB)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from -r natural_language_processing/arabic_requirements.txt (line 4)) (2.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r natural_language_processing/arabic_requirements.txt (line 6)) (1.1.5)\n",
            "Collecting elasticsearch\n",
            "  Using cached elasticsearch-7.13.3-py2.py3-none-any.whl (356 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r natural_language_processing/arabic_requirements.txt (line 8)) (1.9.0+cu102)\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (from -r natural_language_processing/arabic_requirements.txt (line 9)) (1.0.61)\n",
            "Collecting fastbook\n",
            "  Using cached fastbook-0.0.16-py3-none-any.whl (720 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r natural_language_processing/arabic_requirements.txt (line 11)) (0.11.1)\n",
            "Collecting tensorflow_gpu>=2.0\n",
            "  Using cached tensorflow_gpu-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
            "Collecting ktrain\n",
            "  Using cached ktrain-0.26.5.tar.gz (25.3 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r natural_language_processing/arabic_requirements.txt (line 16)) (4.41.1)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (21.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (0.22.2.post1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (0.10.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (0.8.9)\n",
            "Collecting networkx<2.4,>=2.2\n",
            "  Using cached networkx-2.3.zip (1.7 MB)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph==0.8.2->-r natural_language_processing/arabic_requirements.txt (line 15)) (3.2.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (57.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (3.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r natural_language_processing/arabic_requirements.txt (line 6)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r natural_language_processing/arabic_requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.34.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (2.5.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.12.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (3.17.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.6.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (3.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (4.6.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph==0.8.2->-r natural_language_processing/arabic_requirements.txt (line 15)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph==0.8.2->-r natural_language_processing/arabic_requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph==0.8.2->-r natural_language_processing/arabic_requirements.txt (line 15)) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx<2.4,>=2.2->stellargraph==0.8.2->-r natural_language_processing/arabic_requirements.txt (line 15)) (4.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.0->-r natural_language_processing/arabic_requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow_gpu>=2.0->-r natural_language_processing/arabic_requirements.txt (line 12)) (3.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel-tools->-r natural_language_processing/arabic_requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel-tools->-r natural_language_processing/arabic_requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel-tools->-r natural_language_processing/arabic_requirements.txt (line 2)) (0.3.4)\n",
            "Collecting transformers\n",
            "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel-tools->-r natural_language_processing/arabic_requirements.txt (line 2)) (0.5.3)\n",
            "Collecting camel-kenlm\n",
            "  Using cached camel-kenlm-2020.11.2.tar.gz (250 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->-r natural_language_processing/arabic_requirements.txt (line 17)) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r natural_language_processing/arabic_requirements.txt (line 17)) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r natural_language_processing/arabic_requirements.txt (line 17)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Using cached tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Using cached sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (0.10.0+cu102)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (7.352.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (3.13)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (2.7.3)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (4.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (7.1.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai->-r natural_language_processing/arabic_requirements.txt (line 9)) (1.0.0)\n",
            "Collecting fastai\n",
            "  Using cached fastai-2.4.1-py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (21.1.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (7.6.3)\n",
            "Collecting nbdev>=0.2.38\n",
            "  Using cached nbdev-1.1.14-py3-none-any.whl (46 kB)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "  Using cached fastcore-1.3.20-py3-none-any.whl (53 kB)\n",
            "Collecting ghapi\n",
            "  Using cached ghapi-0.1.19-py3-none-any.whl (51 kB)\n",
            "Requirement already satisfied: jupyter-client<=6.1.12 in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.3.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (4.10.1)\n",
            "Requirement already satisfied: nbconvert<6 in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.6.1)\n",
            "Collecting fastrelease\n",
            "  Using cached fastrelease-0.1.11-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.1.3)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<=6.1.12->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.1.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<=6.1.12->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (22.1.0)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from jupyter-client<=6.1.12->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.0.5)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<=6.1.12->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (4.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (3.3.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->eli5==0.10.1->-r natural_language_processing/arabic_requirements.txt (line 14)) (2.0.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4.0->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4.0->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.2.0)\n",
            "Collecting scikit-learn>=0.18\n",
            "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (5.5.0)\n",
            "Collecting langdetect\n",
            "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (0.42.1)\n",
            "Collecting cchardet\n",
            "  Using cached cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "Collecting syntok\n",
            "  Using cached syntok-1.3.1.tar.gz (23 kB)\n",
            "Collecting seqeval==0.0.19\n",
            "  Using cached seqeval-0.0.19.tar.gz (30 kB)\n",
            "Collecting ktrain\n",
            "  Using cached ktrain-0.26.4.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.26.3.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.26.2.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.26.1.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.26.0.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.25.4.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.25.3.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.25.2.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.25.1.tar.gz (25.3 MB)\n",
            "  Using cached ktrain-0.25.0.tar.gz (25.3 MB)\n",
            "Collecting keras_bert>=0.86.0\n",
            "  Using cached keras-bert-0.88.0.tar.gz (26 kB)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (2.3.3)\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.24.2.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 19 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.24.1.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 18 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.24.0.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 17 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.23.2.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 113 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.23.1.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 103 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.23.0.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 7.8 MB/s \n",
            "\u001b[?25h  Downloading ktrain-0.22.4.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 107 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.22.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 81 kB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting ktrain\n",
            "  Downloading ktrain-0.22.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 101 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.22.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 67 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.22.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 77 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.21.4.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 91 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.21.3.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 7.0 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.21.2.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 4.5 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.21.1.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 4.4 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.21.0.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.1 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.20.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 95 kB/s \n",
            "\u001b[?25hCollecting tensorflow==2.1.0\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8 MB 20 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (4.0.1)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 55.7 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of ktrain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.20.1.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 97 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.20.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 97 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.9.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 80 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.8.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 99 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.7.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 73 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.6.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 70 kB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.18\n",
            "  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 48.2 MB/s \n",
            "\u001b[?25hCollecting cchardet==2.1.5\n",
            "  Downloading cchardet-2.1.5-cp37-cp37m-manylinux1_x86_64.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 51.0 MB/s \n",
            "\u001b[?25hCollecting shap\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 55.4 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of cchardet to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.19.5.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 72 kB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of ktrain to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ktrain-0.19.4.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 67 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 70 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 60 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 88 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.19.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 98 kB/s \n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading ktrain-0.18.5.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 76 kB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of cchardet to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading ktrain-0.18.4.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 72 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.18.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 60 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.18.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 51 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.18.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 63 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.18.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 58 kB/s \n",
            "\u001b[?25hCollecting tensorflow<=2.2.0,>=2.1.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from keras_bert>=0.86.0->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (2.4.3)\n",
            "Collecting keras-transformer>=0.39.0\n",
            "  Downloading keras-transformer-0.39.0.tar.gz (11 kB)\n",
            "Collecting keras-pos-embd>=0.12.0\n",
            "  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n",
            "Collecting keras-multi-head>=0.28.0\n",
            "  Downloading keras-multi-head-0.28.0.tar.gz (14 kB)\n",
            "Collecting keras-layer-normalization>=0.15.0\n",
            "  Downloading keras-layer-normalization-0.15.0.tar.gz (4.2 kB)\n",
            "Collecting keras-position-wise-feed-forward>=0.7.0\n",
            "  Downloading keras-position-wise-feed-forward-0.7.0.tar.gz (4.5 kB)\n",
            "Collecting keras-embed-sim>=0.9.0\n",
            "  Downloading keras-embed-sim-0.9.0.tar.gz (4.1 kB)\n",
            "Collecting keras-self-attention>=0.50.0\n",
            "  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n",
            "Collecting tensorflow<=2.2.0,>=2.1.0\n",
            "  Downloading tensorflow-2.1.4-cp37-cp37m-manylinux2010_x86_64.whl (422.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 422.0 MB 26 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.1.3-cp37-cp37m-manylinux2010_x86_64.whl (421.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 421.9 MB 9.3 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.1.2-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8 MB 24 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8 MB 39 kB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of keras-self-attention to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras-position-wise-feed-forward to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras-pos-embd to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras-multi-head to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras-layer-normalization to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras-embed-sim to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras-transformer to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/keras/\u001b[0m\n",
            "Collecting Keras>=2.4.3\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "INFO: pip is looking at multiple versions of keras-bert to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting keras_bert>=0.81.0\n",
            "  Downloading keras-bert-0.87.0.tar.gz (26 kB)\n",
            "  Downloading keras-bert-0.86.0.tar.gz (26 kB)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading keras-transformer-0.38.0.tar.gz (11 kB)\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading keras-position-wise-feed-forward-0.6.0.tar.gz (4.4 kB)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading keras-pos-embd-0.11.0.tar.gz (5.9 kB)\n",
            "INFO: pip is looking at multiple versions of keras-self-attention to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of keras-position-wise-feed-forward to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading keras-multi-head-0.27.0.tar.gz (14 kB)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading keras-self-attention-0.46.0.tar.gz (10 kB)\n",
            "INFO: pip is looking at multiple versions of keras-pos-embd to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading keras-layer-normalization-0.14.0.tar.gz (4.3 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "INFO: pip is looking at multiple versions of keras-multi-head to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading keras-embed-sim-0.8.0.tar.gz (4.1 kB)\n",
            "INFO: pip is looking at multiple versions of keras-layer-normalization to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "INFO: pip is looking at multiple versions of keras-embed-sim to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "INFO: pip is looking at multiple versions of keras-transformer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting keras_bert>=0.81.0\n",
            "  Downloading keras-bert-0.85.0.tar.gz (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "INFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading keras-bert-0.84.0.tar.gz (27 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "Collecting keras-transformer>=0.37.0\n",
            "  Downloading keras-transformer-0.37.0.tar.gz (11 kB)\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading keras-embed-sim-0.7.0.tar.gz (4.1 kB)\n",
            "Collecting keras_bert>=0.81.0\n",
            "  Downloading keras-bert-0.83.0.tar.gz (27 kB)\n",
            "Collecting keras-transformer>=0.35.0\n",
            "  Downloading keras-transformer-0.36.0.tar.gz (11 kB)\n",
            "Collecting keras-multi-head>=0.26.0\n",
            "  Downloading keras-multi-head-0.26.0.tar.gz (14 kB)\n",
            "Collecting keras-self-attention==0.44.0\n",
            "  Downloading keras-self-attention-0.44.0.tar.gz (11 kB)\n",
            "Collecting keras-transformer>=0.35.0\n",
            "  Downloading keras-transformer-0.35.0.tar.gz (11 kB)\n",
            "Collecting keras-multi-head>=0.25.0\n",
            "  Downloading keras-multi-head-0.25.0.tar.gz (14 kB)\n",
            "Collecting keras-self-attention==0.43.0\n",
            "  Downloading keras-self-attention-0.43.0.tar.gz (11 kB)\n",
            "Collecting keras_bert>=0.81.0\n",
            "  Downloading keras-bert-0.82.0.tar.gz (27 kB)\n",
            "Collecting keras-transformer>=0.34.0\n",
            "  Downloading keras-transformer-0.34.0.tar.gz (11 kB)\n",
            "Collecting keras-multi-head>=0.24.0\n",
            "  Downloading keras-multi-head-0.24.0.tar.gz (12 kB)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading keras-self-attention-0.41.0.tar.gz (9.3 kB)\n",
            "Collecting keras_bert>=0.81.0\n",
            "  Downloading keras-bert-0.81.1.tar.gz (28 kB)\n",
            "Collecting keras-transformer==0.33.0\n",
            "  Downloading keras-transformer-0.33.0.tar.gz (11 kB)\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading keras-position-wise-feed-forward-0.5.0.tar.gz (4.0 kB)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading keras-pos-embd-0.10.0.tar.gz (5.2 kB)\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading keras-multi-head-0.22.0.tar.gz (12 kB)\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading keras-layer-normalization-0.13.0.tar.gz (4.0 kB)\n",
            "  Downloading keras-layer-normalization-0.12.0.tar.gz (4.0 kB)\n",
            "INFO: pip is looking at multiple versions of keras-bert to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting keras_bert>=0.81.0\n",
            "  Downloading keras-bert-0.81.0.tar.gz (29 kB)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading keras-transformer-0.32.0.tar.gz (11 kB)\n",
            "  Downloading keras-transformer-0.31.0.tar.gz (12 kB)\n",
            "  Downloading keras-transformer-0.30.0.tar.gz (11 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.17.5.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 43 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.17.4.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 32 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.17.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading ktrain-0.17.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 30 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.17.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 41 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.17.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 41 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.16.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 16 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.16.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 15 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.16.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 17 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.16.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 15 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.15.4.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 3.8 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.15.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 822 bytes/s \n",
            "\u001b[?25h  Downloading ktrain-0.15.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 111 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.15.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 112 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.15.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 66 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.7.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 92 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.6.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.5.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 82 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.4.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 55 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 4.7 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 65.9 MB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 113 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.14.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 79 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.13.2.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 68 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.13.1.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 70 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.13.0.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 48 kB/s \n",
            "\u001b[?25h  Downloading ktrain-0.12.3.tar.gz (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 85 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert<6->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.10.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (1.7.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (5.1.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nbdev>=0.2.38->fastbook->-r natural_language_processing/arabic_requirements.txt (line 10)) (1.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r natural_language_processing/arabic_requirements.txt (line 17)) (7.1.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (5.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (1.1.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (0.1.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->ktrain->-r natural_language_processing/arabic_requirements.txt (line 13)) (1.53.0)\n",
            "Building wheels for collected packages: en-core-web-sm, eli5, stellargraph, networkx, camel-tools, ktrain, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, camel-kenlm, langdetect, seqeval, syntok\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-py3-none-any.whl size=12019123 sha256=57afe5598541e8701c33d84707592304876e0a5411520c141cf846451acca376\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/69/41/6f820cf1d7488a0381a2059f66ec9f8f23116f7c67d18f3d8d\n",
            "  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eli5: filename=eli5-0.10.1-py2.py3-none-any.whl size=106850 sha256=5fb233adfb9ec31e03b8bceb7291dd67328b97b6f9aa65f98ecbe7b145ce9b05\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2r_qorn2/wheels/f8/8c/47/b4cafd3c9519194a489dd614ead1485899d2b34c5adc4198ff\n",
            "  Building wheel for stellargraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stellargraph: filename=stellargraph-0.8.2-py3-none-any.whl size=146390 sha256=89bffd3edadd542bb34629e2c485f04ef9b5cbd337c0ea13b23d333f64bd2f1b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2r_qorn2/wheels/4e/67/1a/14af19d488e0925fa17af83a3ce27014e6ae1531f7f11a18cb\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556007 sha256=3c147a00942a8195ce709df51815ecbad4d3c5426f48ec03c71c9e426466494c\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/e6/b8/4efaab31158e9e9ca9ed80b11f6b11130bac9a9672b3cbbeaf\n",
            "  Building wheel for camel-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-tools: filename=camel_tools-1.1.0-py3-none-any.whl size=96907 sha256=fd27116472fafea2f55cd09070813abd30b1b458ab803a0871aed570dc3b45db\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/9f/4a/43286ef26748f0831defc8f590926d30c22b210853e1e4af89\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.12.3-py3-none-any.whl size=25228499 sha256=23d8244672f039dbff3f656847ecf5be7511a2572f7157a477a94b0ba9a528b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/b3/93/58b9d550c2076c302f7e15e6e8d3806cc75686586f3c4ee06d\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.88.0-py3-none-any.whl size=34205 sha256=b5ebd8b01a7f00189dd2dc5283466deef31b8397b7fad917003db892937b3c18\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/90/cd/c038f2366929a3a5e3414a303b673e10235e802d871d29a835\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.39.0-py3-none-any.whl size=12840 sha256=9fa3618abb092e3d8e64f89e856b7eb4e7492a6e5334d3cedf5b9f72b73d320d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/01/e0/5a1a14bed6726f2ed73f7917d2d2c2d4081d2c88426dea07ce\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.9.0-py3-none-any.whl size=4506 sha256=f8bed5630fbcb0ec554b2fdcc34b25fcb2d741b5667a1bb126d50c107fa17f1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1e/d2/9bc15513dd2f8b9de3e628b3aa9d2de49e721deef6bbd1497e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.15.0-py3-none-any.whl size=5224 sha256=90208dac47c8eb8e9307e9be498a79bd5a8c3b3be25edd8c90ea4a6ec2c3ac0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/be/fe/55422f77ac11fe6ddcb471198038de8a26b5a4dd1557883c1e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.28.0-py3-none-any.whl size=15560 sha256=9dbabbd8f337bb9224cd0ad1f170e8021d65edd492e1fdfbf457ca76bcc1c08a\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/4a/ea/9503ab5a02201dfb8635ba2cc8f30844661623c684a5b44472\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7470 sha256=8d32cb44f17603f3e783517b5a9d37666132966d70e5f063124d1c423bbb97fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.7.0-py3-none-any.whl size=5541 sha256=db93fb3933a1eba4798eb1f5c0bacc9458697ea54e8eee4df735a511caa8c36b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/12/02/1ad455c4f181cda1a4e60c5445855853d5c2ea91f942586a04\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19415 sha256=ad179ed810473d66ee3facde74ddcaefecb22f12819be53673c335935556e867\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n",
            "  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2020.11.2-cp37-cp37m-linux_x86_64.whl size=2309845 sha256=b4714454f358426f14ee1a3d580bb729a57c7682e83b6b4d628654e8efba5c56\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/be/67/7122f437e5a4328499c80d9b4b5b6a064a3501cf24d3414087\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=0fe8158910d2ddeb5d18a5fe13c195f812d0a7401eebeab6244df682793b0cb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=a637c812269e66b428e837c4d857eb398045ba3327d79273f82ae8b859d8e2cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20917 sha256=ad931963a01be3af8faa825e9baeeb0c6ff674fe0ee7eb7c5b9dc445ce996af0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/c2/33/e5d7d8f2f8b0c391d76bf82b844c3151bf23a84d75d02b185f\n",
            "Successfully built en-core-web-sm eli5 stellargraph networkx camel-tools ktrain keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention camel-kenlm langdetect seqeval syntok\n",
            "Installing collected packages: keras-self-attention, fastcore, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, ghapi, tokenizers, sentencepiece, sacremoses, keras-transformer, fastrelease, transformers, syntok, seqeval, networkx, nbdev, langdetect, keras-bert, fastai, cchardet, camel-kenlm, tensorflow-gpu, stellargraph, stanza, ktrain, fastbook, en-core-web-sm, eli5, elasticsearch, camel-tools\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 2.5.1\n",
            "    Uninstalling networkx-2.5.1:\n",
            "      Successfully uninstalled networkx-2.5.1\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed camel-kenlm-2020.11.2 camel-tools-1.1.0 cchardet-2.1.7 elasticsearch-7.13.3 eli5-0.10.1 en-core-web-sm-2.2.0 fastai-2.4.1 fastbook-0.0.16 fastcore-1.3.20 fastrelease-0.1.11 ghapi-0.1.19 keras-bert-0.88.0 keras-embed-sim-0.9.0 keras-layer-normalization-0.15.0 keras-multi-head-0.28.0 keras-pos-embd-0.12.0 keras-position-wise-feed-forward-0.7.0 keras-self-attention-0.50.0 keras-transformer-0.39.0 ktrain-0.12.3 langdetect-1.0.9 nbdev-1.1.14 networkx-2.3 sacremoses-0.0.45 sentencepiece-0.1.96 seqeval-1.2.2 stanza-1.2.2 stellargraph-0.8.2 syntok-1.3.1 tensorflow-gpu-2.5.0 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNLp8KHK5Xgc"
      },
      "source": [
        "Go to the practice directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCpHThFZ5LGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187e69d5-7730-40d5-b3fe-5bc0b66ff6cd"
      },
      "source": [
        "%cd natural_language_processing/text_classification_practice_dl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/natural_language_processing/text_classification_practice_dl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqBBIxf03lS"
      },
      "source": [
        "Ensure that you have the GPU runtime activated:\n",
        "\n",
        "![](https://miro.medium.com/max/3006/1*vOkqNhJNl1204kOhqq59zA.png)\n",
        "\n",
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBCzbwbcSmGF"
      },
      "source": [
        "# HARD: Hotel Arabic-Reviews Dataset\n",
        "\n",
        "For this practice, we focus on applying the text classification techniques covered so far to a dataset in Arabic.\n",
        "\n",
        "In particular, we are using the [HARD: Hotel Arabic-Reviews Dataset](https://github.com/elnagara/HARD-Arabic-Dataset). This dataset contains 93700 hotel reviews in the Arabic language. The hotel reviews were collected from Booking.com website during June/July 2016. The reviews are expressed in Modern Standard Arabic as well as dialectal Arabic.\n",
        "This dataset represents a common NLP challenge:  you have a set of consumer reviews that you want to analyze automatically (e.g., discover the sentiment, find the most relevant topics, ...). \n",
        "\n",
        "Previously, we have experimented with the original version of BERT (i.e., the one trained by Google). Although BERT has represented a considerable step ahead in the NLP research, its application is limited to English content. Since then, several initiatives have adapted the BERT architecture to contents in other languages. For this practice, we will make use of one of them: AraBERT.\n",
        "\n",
        "In the following, we will download both the HARD dataset and the AraBERT pre-trained model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlMoq-hmZ61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53deafe7-7ef9-463a-b276-899cfa12b829"
      },
      "source": [
        "# Download the AraBERT pre-trained model\n",
        "!git clone https://github.com/aub-mind/arabert\n",
        "\n",
        "# Download and uzip the HARD dataset\n",
        "!git clone https://github.com/elnagara/HARD-Arabic-Dataset\n",
        "!unrar x 'HARD-Arabic-Dataset/data/unbalanced-reviews.rar'\n",
        "!unzip 'HARD-Arabic-Dataset/data/balanced-reviews.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'arabert' already exists and is not an empty directory.\n",
            "fatal: destination path 'HARD-Arabic-Dataset' already exists and is not an empty directory.\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from HARD-Arabic-Dataset/data/unbalanced-reviews.rar\n",
            "\n",
            "Extracting  unbalanced-reviews.txt                                       \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "Archive:  HARD-Arabic-Dataset/data/balanced-reviews.zip\n",
            "  inflating: balanced-reviews.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVay9KamnC3I"
      },
      "source": [
        "## Reading and preparing the dataset.\n",
        "\n",
        "To enable the experimentation, we read the dataset and split the dataset into training and test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krBvefg6l6vv"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr84ozGinCFh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "all_datasets= []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PhWP2JzrEci"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name,\n",
        "        train,\n",
        "        test,\n",
        "        label_list,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WYy5ikAs7l3"
      },
      "source": [
        "DATA_COLUMN = \"text\"\n",
        "LABEL_COLUMN = \"label\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6oL3qkXmOgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8eebecc-4e56-4360-bcf6-803f346dfd36"
      },
      "source": [
        "df_HARD = pd.read_csv(\"balanced-reviews.txt\", sep=\"\\t\", header=0,encoding='utf-16')\n",
        "\n",
        "df_HARD = df_HARD[[\"review\",\"rating\"]]  # we are interested in rating and review only\n",
        "df_HARD.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "print(df_HARD[LABEL_COLUMN].value_counts())\n",
        "# code rating as +ve if > 3, -ve if less, no 3s in dataset\n",
        "\n",
        "hard_map = {\n",
        "    5: 'POS',\n",
        "    4: 'POS',\n",
        "    2: 'NEG',\n",
        "    1: 'NEG'\n",
        "}\n",
        "\n",
        "df_HARD[LABEL_COLUMN] = df_HARD[LABEL_COLUMN].apply(lambda x: hard_map[x])\n",
        "train_HARD, test_HARD = train_test_split(df_HARD, test_size=0.2, random_state=42)\n",
        "label_list_HARD = ['NEG', 'POS']\n",
        "\n",
        "data_Hard = Dataset(\"HARD\", train_HARD, test_HARD, label_list_HARD)\n",
        "all_datasets.append(data_Hard)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2    38467\n",
            "4    26450\n",
            "5    26399\n",
            "1    14382\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcwdslw7v0Q8"
      },
      "source": [
        "# Modeling\n",
        "\n",
        "Since AraBERT is not included in ktrain or other high-level libraries, we need some previous work to load the model and fine-tune it with the HARD dataset.\n",
        "\n",
        "Luckily, HuggingFace provides a \"handy\" way of loading pre-trained models for many different NLP tasks. I recommend you check the HuggingFace documentation for more details on applying Deep Learning models to different scenarios and languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUn2RB6Bvrxj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23926efc-6142-4d81-9f5f-20e793b06615"
      },
      "source": [
        "!pip install pyarabic\n",
        "!pip install optuna==2.3.0\n",
        "!pip install transformers==4.2.1\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import logging\n",
        "import torch\n",
        "import optuna "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (0.6.11)\n",
            "Collecting optuna==2.3.0\n",
            "  Downloading optuna-2.3.0.tar.gz (258 kB)\n",
            "\u001b[K     |████████████████████████████████| 258 kB 36.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.8.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "  Downloading alembic-1.6.5-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.19.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (4.41.1)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.4.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna==2.3.0) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (4.6.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==2.3.0) (2.8.1)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (3.13)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (2.1.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.1.2-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.3.0) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==2.3.0) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna==2.3.0) (1.15.0)\n",
            "Building wheels for collected packages: optuna, pyperclip\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-py3-none-any.whl size=359773 sha256=781bc597c6a61ac770ca3f467ff077e1fb5c871d53e9b03a6ef50416c4c11b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/61/9e/955ab1890f6cab231b1d756db63f36c711968a324296e0b649\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=7756fa17878058300abb51953b1f13f7c8c2e48822253a0c4beefc26751242db\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built optuna pyperclip\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, python-editor, Mako, cmd2, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.4 alembic-1.6.5 cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.2 colorama-0.4.4 colorlog-5.0.1 optuna-2.3.0 pbr-5.6.0 pyperclip-1.8.2 python-editor-1.0.4 stevedore-3.3.0\n",
            "Collecting transformers==4.2.1\n",
            "  Downloading transformers-4.2.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 32.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (21.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.1) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.1) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.15.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "camel-tools 1.1.0 requires transformers==3.0.2, but you have transformers 4.2.1 which is incompatible.\u001b[0m\n",
            "Successfully installed tokenizers-0.9.4 transformers-4.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfNKr05tv7cA"
      },
      "source": [
        "logging.basicConfig(level=logging.WARNING)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4SGYoB2EDJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7ccdf3-4fcf-4baf-c1d2-1936331a1941"
      },
      "source": [
        "for x in all_datasets:\n",
        "  print(x.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HARD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Lma6tT5zJi"
      },
      "source": [
        "You can choose which model, and dataset from here along with the max sentence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzeVFoz1wDYf"
      },
      "source": [
        "dataset_name = 'HARD'\n",
        "model_name = 'aubmindlab/bert-base-arabertv02'\n",
        "task_name = 'classification'\n",
        "max_len = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x0O4-tcWuxV"
      },
      "source": [
        "The first thing to do is define a pre-processor for reading and format the data in a way that AraBERT can use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt_lGy85zuca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797923b6-71f9-41bd-af93-df8ef37418ea"
      },
      "source": [
        "arabert_prep = ArabertPreprocessor(model_name.split(\"/\")[-1])\n",
        "\n",
        "selected_dataset.train[DATA_COLUMN] = selected_dataset.train[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(x))\n",
        "selected_dataset.test[DATA_COLUMN] = selected_dataset.test[DATA_COLUMN].apply(lambda x:   arabert_prep.preprocess(x))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoyaRIGYCEzh",
        "outputId": "f010bb77-b91f-469a-8b63-367c96c9867b"
      },
      "source": [
        "selected_dataset.test[DATA_COLUMN]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28739     استثنائي . كل شيء . الخروج المبكراتصلو علي الس...\n",
              "98861               استثنائي . . صوت الديسكو مزعج والطيارات\n",
              "23662                              “ ازعاج ” . لاشي . كل شي\n",
              "47107     “ لا انصح ابدا بزيارته ” . لم يعجبني شي . النظ...\n",
              "3282      “ ممتعة ” . الهدوء و الخصوصية . قمت بطلب تزيين...\n",
              "                                ...                        \n",
              "85626     ضعيف . لاشيء . لايوجد مواقف سيارات المطعم لايت...\n",
              "14421     “ عاديه واقل من عاديه ” . الافطار الاصناف قليل...\n",
              "100923    ضعيف . يوجد باصات الى الحرم . - عدم نظافة الفن...\n",
              "88655     “ عدم الالتزام بمنح الغرف التي يتم حجزها على ب...\n",
              "59742     ضعيف . . يجب الإهتمام بالنظافة . . الأرضيات ال...\n",
              "Name: text, Length: 21140, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJYs1xhdXU3V"
      },
      "source": [
        "Now we will tokenize the data and perform some additional pre-processing steps so that AraBERT can ingest the input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YS7XI2bZTyz"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\n",
        "      super(BERTDataset).__init__()\n",
        "      self.text = text\n",
        "      self.target = target\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "      \n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "        \n",
        "      input_ids = self.tokenizer.encode(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          truncation='longest_first'\n",
        "      )     \n",
        "    \n",
        "      attention_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = self.max_len - len(input_ids)\n",
        "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
        "      attention_mask = attention_mask + ([0] * padding_length)    \n",
        "      \n",
        "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mciZOFz-amkV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "b03bf73a-b15e-4c04-cb40-b2bfb200230d"
      },
      "source": [
        "label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\n",
        "print(label_map)\n",
        "train_dataset = BERTDataset(selected_dataset.train[DATA_COLUMN].to_list(),selected_dataset.train[LABEL_COLUMN].to_list(),model_name,max_len,label_map)\n",
        "test_dataset = BERTDataset(selected_dataset.test[DATA_COLUMN].to_list(),selected_dataset.test[LABEL_COLUMN].to_list(),model_name,max_len,label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'NEG': 0, 'POS': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;31m# Name or path to the pretrained checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5069d75f0ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABEL_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABEL_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0afbed0ac2b1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text, target, model_name, max_len, label_map)\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't set {} with value {} for {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load config for 'aubmindlab/bert-base-arabertv02'. Make sure that:\n\n- 'aubmindlab/bert-base-arabertv02' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'aubmindlab/bert-base-arabertv02' is the correct path to a directory containing a config.json file\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9hKg2uSXndG"
      },
      "source": [
        "The following code loads the weights of the pre-trained AraBERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt7l0IxjbmNu"
      },
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMZ5ObVbXu6D"
      },
      "source": [
        "In order to facilitate the model evaluation, the following function computes several classification metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYU6G4vWc5nz"
      },
      "source": [
        "def compute_metrics(p): #p should be of type EvalPrediction\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  assert len(preds) == len(p.label_ids)\n",
        "  #print(classification_report(p.label_ids,preds))\n",
        "  #print(confusion_matrix(p.label_ids,preds))\n",
        "\n",
        "  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
        "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
        "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
        "  acc = accuracy_score(p.label_ids,preds)\n",
        "  return {\n",
        "      'macro_f1' : macro_f1,\n",
        "      'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
        "      'macro_precision': macro_precision,\n",
        "      'macro_recall': macro_recall,\n",
        "      'accuracy': acc\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTmvFEs41WkV"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_oGjIC-7Vow"
      },
      "source": [
        "In the following, we will finally fine-tune the model with the HARD dataset.\n",
        "\n",
        "I am using some default values for the hyper-parameters. I leave as an exercise to you the hyper-parameter optimization if you want to improve the model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xjs-X14uc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "0241da61-efad-4dc1-c78a-95ca389edb05"
      },
      "source": [
        "training_args = TrainingArguments(\"./train\")\n",
        "training_args.evaluate_during_training = True\n",
        "training_args.adam_epsilon = 1e-8\n",
        "training_args.learning_rate = 5e-5\n",
        "training_args.fp16 = True\n",
        "training_args.per_device_train_batch_size = 16\n",
        "training_args.per_device_eval_batch_size = 16\n",
        "training_args.gradient_accumulation_steps = 2\n",
        "training_args.num_train_epochs= 8\n",
        "\n",
        "\n",
        "steps_per_epoch = (len(selected_dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "print(steps_per_epoch)\n",
        "print(total_steps)\n",
        "#Warmup_ratio\n",
        "warmup_ratio = 0.1\n",
        "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
        "\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
        "# training_args.logging_steps = 200\n",
        "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
        "training_args.seed = 42\n",
        "training_args.disable_tqdm = False\n",
        "training_args.lr_scheduler_type = 'cosine'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2642\n",
            "21136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-feb424415063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwarmup_ratio\u001b[0m \u001b[0;31m# or you can set the warmup steps directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluationStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# training_args.logging_steps = 200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m \u001b[0;31m#don't want to save any model, there is probably a better way to do this :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EvaluationStrategy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro5BW5ak4uc1"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = model_init(),\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx336O3J2SdQ"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}