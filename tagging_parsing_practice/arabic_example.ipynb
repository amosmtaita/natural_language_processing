{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "CAMeL_Tools_Guided_Tour.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/natural_language_processing/blob/master/tagging_parsing_practice/arabic_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvwbQnTvBRh"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**\n",
        "\n",
        "The first step is to clone the repository to have access to all the data and files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d7mC64KvlwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54aef90-aeb1-4f32-a0a6-2657e2e6f099"
      },
      "source": [
        "! git clone https://github.com/acastellanos-ie/natural_language_processing.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'natural_language_processing'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecfec2Y4v6e9"
      },
      "source": [
        "Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIep7l0jvtUB"
      },
      "source": [
        "! pip install -Uqqr natural_language_processing/arabic_requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDzMQwpyODo"
      },
      "source": [
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c8-zs9m45B0"
      },
      "source": [
        "In order to use all the components provided in CAMeL Tools, we need to install all the datasets required by these components.\n",
        "To do this in Colab, we need to first mount a Google Drive and create a directory where the data will be installed.\n",
        "\n",
        "Run the code below and follow the instructions in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLDEdYgz1OZN"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%mkdir /gdrive/MyDrive/camel_tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcc0ZaAK5f5F"
      },
      "source": [
        "Next, we need to tell CAMeL Tools to install the data in the newly created directory. This will take a couple of minutes to complete.\n",
        "\n",
        "**NOTE:** You will need at least 2.3GB of available space on your Google Drive to install all the CAMeL Tools data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYsFJl0A5mua"
      },
      "source": [
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'\n",
        "\n",
        "!export | camel_data full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW2oOVmkJgg-"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook provides a quick overview of the different pre-processing steps needed to deal with textual contents in Arabic. It does not intend to be a full review but to provide you with illustrative examples and pointers to additional materials.\n",
        "\n",
        "To that end, I have used the guidelines and functionalities provided by [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools), one of the libraries that I recommended you to check.\n",
        "\n",
        "For more detailed information please refer to the [CAMeL Tools documentation](https://camel-tools.readthedocs.io/en/latest/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhD5hMgA088t"
      },
      "source": [
        "# Dediacritization\n",
        "\n",
        "Dediacritization is the process of removing Arabic diacritical marks. \n",
        "\n",
        "As discussed in class, Diacritics increase data sparsity and so most Arabic NLP techniques ignore them. The example below shows how diacritics can be removed from Arabic text using the [`dediac_ar`](https://camel-tools.readthedocs.io/en/latest/api/utils/dediac.html#camel_tools.utils.dediac.dediac_ar) function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np_JWmFq088u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd3de1a-00d5-473d-8967-0d99293fae1d"
      },
      "source": [
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
        "print(sentence)\n",
        "\n",
        "sent_dediac = dediac_ar(sentence)\n",
        "print(sent_dediac)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
            "هل ذهبت إلى المكتبة؟\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQw096Uw088u"
      },
      "source": [
        "# Tokenization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Dr0J4W_-Bp"
      },
      "source": [
        "## Word Tokenization\n",
        "\n",
        "Tokenization is the step to split the entire sentence into the individual units of meaning (i.e., words). In class, we reviewed the standard process and the challenges it posed to Arabic.\n",
        "\n",
        "In this sense, there are different tokenization strategies that we can follow.\n",
        "\n",
        "Python does provide the `split()` method to tokenize words by whitespace, but it doesn't separate punctuation from words. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqLIIGGim5nm",
        "outputId": "bca8345e-38d1-4de2-dbcf-77988d1ddc4d"
      },
      "source": [
        "sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
        "print(sentence)\n",
        "\n",
        "sent_split = sentence.split()\n",
        "print(sent_split)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
            "['هَلْ', 'ذَهَبْتَ', 'إِلَى', 'المَكْتَبَةِ؟']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sju7VQ6m3h5"
      },
      "source": [
        "A similar function to split by whitespace and separate punctuation is also provided by CAMeL Tools [`simple_word_tokenize`](https://camel-tools.readthedocs.io/en/latest/api/tokenizers/word.html#camel_tools.tokenizers.word.simple_word_tokenize).\n",
        "\n",
        "The example below is similar to the one above, but this time we use `simple_word_tokenize()` instead of `split()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxbPNRCU088v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7c4f48-75fd-4378-effb-e333e2d58738"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
        "print(sentence)\n",
        "\n",
        "sent_split = simple_word_tokenize(sentence)\n",
        "print(sent_split)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
            "['هَلْ', 'ذَهَبْتَ', 'إِلَى', 'المَكْتَبَةِ', '؟']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uuXeQjd0889"
      },
      "source": [
        "## Morphological Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeKhpPVsI69"
      },
      "source": [
        "The tokenization strategies mentioned above are simply splitting sentences by whitespace and separating punctuation. However, this is not the best choice for Arabic. In contrast, morphological tokenization splits Arabic words into component prefixes, stems, and suffixes.\n",
        "\n",
        "CAMeL Tools provides the [`MorphologicalTokenizer`](https://camel-tools.readthedocs.io/en/latest/api/tokenizers/morphological.html#camel_tools.tokenizers.morphological.MorphologicalTokenizer) class to tokenize words in different schemes. It behaves very much like the `DefaultTagger` in that it uses a disambiguator first to disambiguate words and then extracts a particular tokenization feature, but it has the following differences:\n",
        "\n",
        "- While the `DefaultTagger` produces exactly one output for each input word, the `MorphologicalTokenizer` might produce multiple output tokens.\n",
        "-  The `MorphologicalTokenizer` can be configured to produce diacritized and undiacritized output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upf_eeVE0889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b676e705-4d91-4797-b47d-c21012be279a"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
        "\n",
        "# The tokenizer expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('فتنفست الصعداء')\n",
        "print(sentence)\n",
        "\n",
        "# Load a pretrained disambiguator to use with a tokenizer\n",
        "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "\n",
        "# Without providing additional arguments, the tokenizer will output undiacritized\n",
        "# morphological tokens for each input word delimited by an underscore.\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok')\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "# By specifying `split=True`, the morphological tokens are output as seperate\n",
        "# strings.\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "# We can output diacritized tokens by setting `diac=True`\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['فتنفست', 'الصعداء']\n",
            "['ف+_تنفست', 'ال+_صعداء']\n",
            "['ف+', 'تنفست', 'ال+', 'صعداء']\n",
            "['فَ+', 'تَنَفَّسْتُ', 'ال+', 'صُعَداءَ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMTRdw4g0888"
      },
      "source": [
        "# Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYy16RApKQX"
      },
      "source": [
        "This step focuses on identifying the role of a given word in the context of a particular sentence. \n",
        "\n",
        "CAMeL Tools provides a [`DefaultTagger`](https://camel-tools.readthedocs.io/en/latest/api/tagger/default.html#camel_tools.tagger.default.DefaultTagger) for the POS Tagging of Arabic contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ToGeNYB0889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359ec837-6742-464e-e13d-cf67e9a18694"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tagger.default import DefaultTagger\n",
        "\n",
        "mle = MLEDisambiguator.pretrained()\n",
        "tagger = DefaultTagger(mle, 'pos')\n",
        "\n",
        "# The tagger expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('نجح بايدن في الانتخابات')\n",
        "\n",
        "pos_tags = tagger.tag(sentence)\n",
        "\n",
        "print(pos_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['verb', 'noun_prop', 'prep', 'noun']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jo6RMNz088_"
      },
      "source": [
        "# Named Entitiy Recognition\n",
        "\n",
        "Finally, CAMeL Tools comes with an easy-to-use, pretrained [named-entitity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)  (NER) system that can be accessed using the [`NERecognizer`](https://camel-tools.readthedocs.io/en/latest/api/ner.html#camel_tools.ner.NERecognizer) class. \n",
        "\n",
        "For each token in an input sentence, `NERecognizer` outputs a label that indicates the type of named-entity. The system outputs one of the following labels for each token: `'B-LOC'`, `'B-ORG'`, `'B-PERS'`, `'B-MISC'`, `'I-LOC'`, `'I-ORG'`, `'I-PERS'`, `'I-MISC'`, `'O'`.\n",
        "Named-entites can either be a `LOC` (location), `ORG` (organization), `PERS` (person), or `MISC` (miscallaneous).\n",
        "\n",
        "Labels beginning with `B` indicate that their corresponding tokens are the beginning of a multi-word named-entity or are a single-token named-entity'. Those begining with `I` indicate that their corresponding tokens are continuations of a multi-word named-entity. Words that aren't named-entities are given the `'O'` label.\n",
        "\n",
        "The example below illustrates how `NERecognizer` can be used to label named-entities in a given sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwBGDtiY088_",
        "outputId": "a12b11ae-e87f-438e-95cc-712d045ddcb8"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.ner import NERecognizer\n",
        "\n",
        "ner = NERecognizer.pretrained()\n",
        "\n",
        "# NERecognizer expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع.')\n",
        "\n",
        "labels = ner.predict_sentence(sentence)\n",
        "\n",
        "# Print each token paired with it's NER label\n",
        "print(list(zip(sentence, labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('إمارة', 'O'), ('أبوظبي', 'B-LOC'), ('هي', 'O'), ('إحدى', 'O'), ('إمارات', 'O'), ('دولة', 'O'), ('الإمارات', 'B-LOC'), ('العربية', 'I-LOC'), ('المتحدة', 'I-LOC'), ('السبع', 'O'), ('.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}